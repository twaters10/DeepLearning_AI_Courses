{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51314003-4f36-453f-88ff-c6621b784d59",
   "metadata": {},
   "source": [
    "# Lesson 8 - Hierarchically Chaining the Agent Calls using a Router Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de52b6c",
   "metadata": {},
   "source": [
    "You will now create a hierarchical workflow using a router agent. Instead of having a fixed linear workflow like in the previous lesson, you will use a third agent (router) in the client-side that will decide when to call each ACP agent. </br>\n",
    "Idea is to not have to prompt the individual agents. Just put in one prompt and the system will determine which agent answers which questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c3f95-8f99-4d35-8369-d383176d6bc8",
   "metadata": {},
   "source": [
    "## 8.1. Start Up both ACP Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7a6d8",
   "metadata": {},
   "source": [
    "First make sure the Insurer server is still running:\n",
    "- Open the terminal by running the cell below.\n",
    "- If the agent is still running from the previous lessons, then you don't need to do anything else.\n",
    "- If the agent has stopped running (the lab environment resets after 120 min), then you can run the server again by typing:\n",
    "  - `uv run crew_agent_server.py`\n",
    "\n",
    "\n",
    "**Note**: If you see this warning: \n",
    "`WARNING: Can not reach server, check if running on http://127.0.0.1:8333 : Request failed after 5 retries`\n",
    "you can ignore it. You'll learn later in another lesson about the BeeAI platform, which a registry you can use to manage and discover agents. If the platform is installed, it runs by default on port 8333. The ACP servers are configured to automatically connect to the platform. Since the platform is not installed in this environment, the ACP server will generate a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e350c16f-672e-41f2-80d6-a3b854abc90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://s172-29-54-176p8888.lab-aws-production.deeplearning.ai/terminals/1\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f172fbc7f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "import os\n",
    "url = os.environ.get('DLAI_LOCAL_URL').format(port=8888)\n",
    "IFrame(f\"{url}terminals/1\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c23330",
   "metadata": {},
   "source": [
    "Also make sure the Hospital server is still running:\n",
    "- Open the terminal by running the cell below.\n",
    "- If the agent is still running from the previous lessons, then you don't need to do anything else.\n",
    "- If the agent has stopped running (the lab environment resets after 120 min), then you can run the server again by typing:\n",
    "  - `uv run smolagents_server.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639370d9-5241-4dab-b8cd-67298d9c8ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://s172-29-54-176p8888.lab-aws-production.deeplearning.ai/terminals/2\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f172c3f0bd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(f\"{url}terminals/2\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b648fc-f4f0-45b7-8d02-ecd368aeccc9",
   "metadata": {},
   "source": [
    "## 8.2. Import ACPCallingAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce2605",
   "metadata": {},
   "source": [
    "The router agent is already implemented for you as the `ACPCallingAgent`. You are provided with a python file called `fastacp.py` where you can find the definition of the `ACPCallingAgent`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac5ddc",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> ðŸ’» &nbsp; <b>To access the <code>fastacp.py</code> file:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook, 2) click on <em>\"Open\"</em> and then 3) click on L8.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfeac96-37e7-4e5e-9162-3335e5837cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio \n",
    "import nest_asyncio\n",
    "from acp_sdk.client import Client\n",
    "from smolagents import LiteLLMModel\n",
    "from fastacp import AgentCollection, ACPCallingAgent #AgentCollection structures ACP agents in a format that is usable. ACP Calling then routes the decision to the right agent.\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03d181e-d65e-47d3-83eb-1ba394d3801d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This agent uses JSON-like ACP agent calls, similarly to how ToolCallingAgent uses tool calls,\n",
      "    but directed at remote ACP agents instead of local tools.\n",
      "    \n",
      "    Args:\n",
      "        acp_agents (`dict[str, Agent]`): ACP agents that this agent can call.\n",
      "        model (`Callable[[list[dict[str, str]]], ChatMessage]`): Model that will generate the agent's actions.\n",
      "        prompt_templates ([`Dict[str, str]`], *optional*): Prompt templates.\n",
      "        planning_interval (`int`, *optional*): Interval at which the agent will run a planning step.\n",
      "        **kwargs: Additional keyword arguments.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(ACPCallingAgent.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae3cf3-6c40-4b13-a125-660f21d8f5a4",
   "metadata": {},
   "source": [
    "## 8.3. Run the Hierarchical Workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcbe8308-1bc2-4fd6-88b1-5367ac895e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae8cd09",
   "metadata": {},
   "source": [
    "**Note**: The `fastacp.py` file does not only contain the definition for the ACPCallingAgent, but it also includes this method: `AgentCollection.from_acp` where the client objects (`insurer` and `hospital`) discover the agents hosted on their corresponding servers by calling the method `.agents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96eeae29-1b30-4850-bcc4-27beb65505bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this with open ai models like ollama or something else\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"openai/gpt-4\"\n",
    ")\n",
    "\n",
    "async def run_hospital_workflow() -> None:\n",
    "    async with Client(base_url=\"http://localhost:8001\") as insurer, Client(base_url=\"http://localhost:8000\") as hospital:\n",
    "        # agents discovery\n",
    "        agent_collection = await AgentCollection.from_acp(insurer, hospital)  \n",
    "        acp_agents = {agent.name: {'agent':agent, 'client':client} for client, agent in agent_collection.agents}\n",
    "        print(acp_agents) \n",
    "        # passing the agents as tools to ACPCallingAgent\n",
    "        acpagent = ACPCallingAgent(acp_agents=acp_agents, model=model)\n",
    "        # running the agent with a user query\n",
    "        result = await acpagent.run(\"do i need rehabilitation after a shoulder reconstruction and what is the waiting period from my insurance?\")\n",
    "        print(Fore.YELLOW + f\"Final result: {result}\" + Fore.RESET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca04856a-6404-4e98-9420-bcf8c0e9296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'policy_agent': {'agent': Agent(name='policy_agent', description='This is an agent for questions around policy coverage, it uses a RAG pattern to find answers based on policy documentation. Use it to help answer questions on coverage and waiting periods.', metadata=Metadata(annotations=None, documentation=None, license=None, programming_language=None, natural_languages=None, framework=None, capabilities=None, domains=None, tags=None, created_at=None, updated_at=None, author=None, contributors=None, links=None, dependencies=None, recommended_models=None)), 'client': <acp_sdk.client.client.Client object at 0x7f16fdf70a50>}, 'health_agent': {'agent': Agent(name='health_agent', description='This is a CodeAgent which supports the hospital to handle health based questions for patients. Current or prospective patients can use it to find answers about their health and hospital treatments.', metadata=Metadata(annotations=None, documentation=None, license=None, programming_language=None, natural_languages=None, framework=None, capabilities=None, domains=None, tags=None, created_at=None, updated_at=None, author=None, contributors=None, links=None, dependencies=None, recommended_models=None)), 'client': <acp_sdk.client.client.Client object at 0x7f16fca07e50>}}\n",
      "[INFO] Step 1/10\n",
      "[DEBUG] Output message of the LLM:\n",
      "[DEBUG] ModelResponse(id='chatcmpl-C2svb0DlQDG4gHDdfjXMLksDoAjQ5', created=1754803835, model='gpt-4-0613', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\\n  \"input\": \"Do I need rehabilitation after a shoulder reconstruction?\"\\n}', name='health_agent'), id='call_Ltxw3sEAAzz4jd0QDhyjQjIr', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=20, prompt_tokens=418, total_tokens=438, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "[INFO] Calling agent: 'health_agent' with arguments: {\n",
      "  \"input\": \"Do I need rehabilitation after a shoulder reconstruction?\"\n",
      "}\n",
      "\u001b[33mTool being called with args: ('{\\n  \"input\": \"Do I need rehabilitation after a shoulder reconstruction?\"\\n}',) and kwargs: {'sanitize_inputs_outputs': True}\u001b[39m\n",
      "\u001b[35m{\n",
      "  \"input\": \"Do I need rehabilitation after a shoulder reconstruction?\"\n",
      "}\u001b[39m\n",
      "\u001b[31mrun_id=UUID('55d7d713-8724-4e61-bc5a-12e5bb69872a') agent_name='health_agent' session_id=UUID('db4cc670-d4b9-406e-81ec-598ff5fdff28') status=<RunStatus.FAILED: 'failed'> await_request=None output=[] error=Error(code=<ErrorCode.SERVER_ERROR: 'server_error'>, message='Invalid yield') created_at=datetime.datetime(2025, 8, 12, 21, 8, 11, 522321, tzinfo=TzInfo(UTC)) finished_at=datetime.datetime(2025, 8, 12, 21, 8, 15, 339261, tzinfo=TzInfo(UTC))\u001b[39m\n",
      "[ERROR] Error executing agent 'health_agent' with arguments \"{\\n  \\\"input\\\": \\\"Do I need rehabilitation after a shoulder reconstruction?\\\"\\n}\": IndexError: list index out of range\n",
      "Please try again or use another agent\n",
      "[ERROR] Error in step 1: Error executing agent 'health_agent' with arguments \"{\\n  \\\"input\\\": \\\"Do I need rehabilitation after a shoulder reconstruction?\\\"\\n}\": IndexError: list index out of range\n",
      "Please try again or use another agent\n",
      "[INFO] Step 2/10\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "[ERROR] Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[ERROR] Error in step 2: Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[INFO] Step 3/10\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "[ERROR] Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[ERROR] Error in step 3: Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[INFO] Step 4/10\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "[ERROR] Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[ERROR] Error in step 4: Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[INFO] Step 5/10\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "[ERROR] Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[ERROR] Error in step 5: Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[INFO] Step 6/10\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "[ERROR] Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[ERROR] Error in step 6: Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[INFO] Step 7/10\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "[ERROR] Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[ERROR] Error in step 7: Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[INFO] Step 8/10\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "[ERROR] Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[ERROR] Error in step 8: Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[INFO] Step 9/10\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "[ERROR] Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[ERROR] Error in step 9: Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[INFO] Step 10/10\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "[ERROR] Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "[ERROR] Error in step 10: Error while generating or parsing output:\n",
      "litellm.RateLimitError: RateLimitError: OpenAIException - exceeded quota for this month\n",
      "\u001b[33mFinal result: I wasn't able to complete this task within the maximum number of steps.\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(run_hospital_workflow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7230a14",
   "metadata": {},
   "source": [
    "**Optional Reading:** Here's how the hierarchical flow works using the provided file `fastacp.py`:\n",
    "\n",
    "1. The agents hosted on each server are first discovered by their corresponding client objects and then converted to tools for the router agent (ACPCallingAgent): \n",
    "  \n",
    "  <img src=\"hr_1.png\" width=\"650\">\n",
    "\n",
    "2. When the router agent receives a user query, it breaks downs the query into smaller steps where each step can be executed by the specialized agent. For a given step, the router agent uses the client of the specialized agent to send the request to it:\n",
    "  \n",
    "  <img src=\"hr_2.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247bd1c5",
   "metadata": {},
   "source": [
    "## 8.4. Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24724b",
   "metadata": {},
   "source": [
    "- [Compose Agents](https://agentcommunicationprotocol.dev/how-to/compose-agents)\n",
    "- [Chained Agents as a Python file](https://github.com/nicknochnack/ACPWalkthrough/blob/main/5.%20Chained%20Agents.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b478c3",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> ðŸ’» &nbsp; <b>To access the <code>my_acp_project</code> folder:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223bf29",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> â¬‡ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
